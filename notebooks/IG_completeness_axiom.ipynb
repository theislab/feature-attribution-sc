{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "exempt-world",
   "metadata": {},
   "outputs": [],
   "source": [
    "from captum.attr import IntegratedGradients\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "academic-moment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a random model\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, num_hidden):\n",
    "        super().__init__()\n",
    "        self.lin1 = nn.Linear(50, num_hidden)\n",
    "        self.lin2 = nn.Linear(num_hidden, num_hidden)\n",
    "        self.lin3 = nn.Linear(num_hidden, 2)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, input):\n",
    "        lin1 = F.relu(self.lin1(input))\n",
    "        lin2 = F.relu(self.lin2(lin1))\n",
    "        lin3 = self.lin3(lin2)\n",
    "        return self.softmax(lin3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "drawn-treat",
   "metadata": {},
   "outputs": [],
   "source": [
    "def completeness_test(model, inp, baseline, attribution, target_class_index, tolerance = 1e-3):\n",
    "    f_inp = model(inp)\n",
    "    f_baseline = model(baseline)\n",
    "    \n",
    "    diff = (f_inp[0][target_class_index] - f_baseline[0][target_class_index]).sum()\n",
    "    attr_sum = attribution.sum()\n",
    "    print(f\"sum of attributions {attr_sum.item()}\")\n",
    "    print(f\"difference of network output at input as baseline {diff.item()}\")\n",
    "    print(f\"approximation error {torch.abs((diff-attr_sum)).item()}\")\n",
    "    \n",
    "    assert torch.abs(attr_sum - diff) <= tolerance, \"failed to pass completness axiom of integrated gradients\"\n",
    "    \n",
    "    print(f\"completness test: passed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "solar-desktop",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create input and baseline\n",
    "inp = torch.arange(0.0, 1.0, 0.02, requires_grad=True).unsqueeze(0)\n",
    "baseline = torch.zeros_like(inp, requires_grad=True)\n",
    "model = Net(20)\n",
    "target_class_index = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surprised-clone",
   "metadata": {},
   "source": [
    "Testing IG as implemented in captum:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "difficult-wedding",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum of attributions 0.0026382685909034052\n",
      "difference of network output at input as baseline 0.002300351858139038\n",
      "approximation error 0.00033791673276436715\n",
      "completness test: passed\n"
     ]
    }
   ],
   "source": [
    "# applying integrated gradients on the SoftmaxModel and input data point\n",
    "ig = IntegratedGradients(model)\n",
    "attributions, approximation_error = ig.attribute(inp, target=target_class_index, return_convergence_delta=True)\n",
    "\n",
    "completeness_test(model, inp, baseline, attributions, target_class_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "connected-firewall",
   "metadata": {},
   "source": [
    "IG as implemented locally (sometimes passes depending on seed, but fails more often than not):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "duplicate-bicycle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum of attributions -0.002332361415028572\n",
      "difference of network output at input as baseline -0.0044172704219818115\n",
      "approximation error 0.0020849090069532394\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "failed to pass completness axiom of integrated gradients",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# target_class_index = 1\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# applying integrated gradients on the SoftmaxModel and input data point\u001b[39;00m\n\u001b[1;32m     14\u001b[0m ig_helmholtz \u001b[38;5;241m=\u001b[39m run_integrated_jacobian_scanvi(model, batches, n_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10000\u001b[39m)\n\u001b[0;32m---> 15\u001b[0m \u001b[43mcompleteness_test\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbaseline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mig_helmholtz\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_class_index\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_class_index\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 11\u001b[0m, in \u001b[0;36mcompleteness_test\u001b[0;34m(model, inp, baseline, attribution, target_class_index, tolerance)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifference of network output at input as baseline \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdiff\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapproximation error \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtorch\u001b[38;5;241m.\u001b[39mabs((diff\u001b[38;5;241m-\u001b[39mattr_sum))\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mabs(attr_sum \u001b[38;5;241m-\u001b[39m diff) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m tolerance, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfailed to pass completness axiom of integrated gradients\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompletness test: passed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAssertionError\u001b[0m: failed to pass completness axiom of integrated gradients"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('..')\n",
    "from scripts.gradients import run_integrated_jacobian_scanvi\n",
    "\n",
    "# Helmholtz method\n",
    "model = Net(20)\n",
    "# num_in = 50\n",
    "inp = torch.arange(0.0, 1.0, 0.02).unsqueeze(0)\n",
    "baseline = torch.zeros_like(inp)\n",
    "\n",
    "batches = [{\"X\":inp, \"batch\":1}]\n",
    "# target_class_index = 1\n",
    "# applying integrated gradients on the SoftmaxModel and input data point\n",
    "ig_helmholtz = run_integrated_jacobian_scanvi(model, batches, n_steps=10000)\n",
    "completeness_test(model, inp, baseline, ig_helmholtz[..., target_class_index], target_class_index)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:fa_base]",
   "language": "python",
   "name": "conda-env-fa_base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
